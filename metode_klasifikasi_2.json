[
    {
        "id": "BW-001",
        "Name of Method": "Pattern Matching (Regex)",
        "Category": "Rule-based",
        "Library/Tools": "Python re module",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Uses predefined regex patterns to classify text based on keyword matches or structures.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Pattern matching with regex involves creating specific patterns using regular expressions to match sequences of characters in text. These patterns can include symbols, word combinations, or positional markers to identify specific textual elements. Regex is widely used for tasks like extracting dates, email addresses, and other structured information from text.",
        "Example Case": "A company filters email addresses from customer feedback by using a regex pattern such as \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b. This enables the organization to extract relevant contacts efficiently.",
        "Is Implemented": true
    },
    {
        "id": "BW-002",
        "Name of Method": "Keyword Matching",
        "Category": "Rule-based",
        "Library/Tools": "Custom scripts",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Searches for specific keywords in text to assign predefined labels.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Keyword matching identifies specific words or phrases within text. It uses a predefined set of keywords to classify or tag content. While straightforward and efficient, this method doesnâ€™t account for contextual relationships between words, which can limit its accuracy in nuanced scenarios.",
        "Example Case": "A review system categorizes customer feedback as positive or negative by identifying keywords like \"excellent\" or \"poor\" in the comments.",
        "Is Implemented": false
    },
    {
        "id": "BW-003",
        "Name of Method": "Boolean Rules",
        "Category": "Rule-based",
        "Library/Tools": "Custom scripts",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Combines logical operators (AND, OR, NOT) to filter and classify text.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Boolean rules apply logical conditions (AND, OR, NOT) to classify or filter text. These rules are flexible and powerful for creating precise conditions but require careful crafting and testing, especially when dealing with diverse or ambiguous input.",
        "Example Case": "An email management tool prioritizes messages containing \"urgent\" AND \"project\" but excludes those with \"spam\" by applying Boolean conditions.",
        "Is Implemented": false
    },
    {
        "id": "BW-004",
        "Name of Method": "Dictionary Lookup",
        "Category": "Rule-based",
        "Library/Tools": "Python Dictionaries",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Maps words to categories based on a predefined dictionary or lexicon.",
        "AI/Non-AI": "Non AI",
        "Long Description": "This method involves mapping words or phrases to predefined categories using a dictionary or lexicon. It is easy to implement and effective for specialized domains but struggles with dynamic or rapidly evolving language usage.",
        "Example Case": "A healthcare system scans patient records to categorize mentions of \"hypertension\" or \"diabetes\" under respective medical conditions.",
        "Is Implemented": true
    },
    {
        "id": "BW-005",
        "Name of Method": "Heuristic Rules",
        "Category": "Rule-based",
        "Library/Tools": "Custom scripts",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Applies domain-specific rules for text classification.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Heuristic rules are handcrafted logic sequences based on domain expertise to classify text. They work well for niche applications where domain-specific patterns dominate but may require regular updates to stay relevant.",
        "Example Case": "A ticketing system tags messages mentioning \"refund\" or \"payment failed\" as finance-related issues.",
        "Is Implemented": false
    },
    {
        "id": "BW-006",
        "Name of Method": "Succeding-Preceding Text",
        "Category": "Rule-based",
        "Library/Tools": "Custom scripts",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Analyzes surrounding context for specific patterns before and after keywords.",
        "AI/Non-AI": "Non AI",
        "Long Description": "This method examines text before and after specific keywords to extract or classify context-sensitive information. It assumes consistent patterns in the data but can struggle with noisy or unstructured input.",
        "Example Case": "Extracting salary information from job postings where phrases like \"Salary: $X/month\" appear consistently, relying on the positioning of keywords.",
        "Is Implemented": true
    },
    {
        "id": "BW-007",
        "Name of Method": "Bag of Words (Simple)",
        "Category": "Statistical",
        "Library/Tools": "Python, Excel",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Counts word frequencies without considering context to classify text.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Bag of Words (BoW) simplifies text into a collection of individual words, ignoring grammar and order. Word frequencies are analyzed for classification. While computationally simple, it fails to capture semantic meaning.",
        "Example Case": "Classifying movie reviews based on the frequency of words like \"amazing\" or \"terrible.\"",
        "Is Implemented": false
    },
    {
        "id": "BW-008",
        "Name of Method": "Term Frequency (TF)",
        "Category": "Statistical",
        "Library/Tools": "Python, Pandas",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Assigns weight to terms based on their occurrence in documents.",
        "AI/Non-AI": "Non AI",
        "Long Description": "TF measures how frequently terms occur within a document. It highlights dominant themes but may overemphasize common words unless combined with other techniques like inverse document frequency.",
        "Example Case": "Analyzing project reports to identify frequently discussed topics by counting the recurrence of terms like \"deadline\" or \"budget.\"",
        "Is Implemented": false
    },
    {
        "id": "BW-009",
        "Name of Method": "Term Frequency-Inverse Document Frequency (TF-IDF)",
        "Category": "Statistical",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Scores terms by their importance across multiple documents.",
        "AI/Non-AI": "Non AI",
        "Long Description": "TF-IDF adjusts term frequency by penalizing common words that appear across multiple documents, highlighting unique terms. This method is widely used in search engines and text feature extraction.",
        "Example Case": "Identifying important terms in customer complaints, such as \"defective\" or \"replacement,\" to prioritize issues.",
        "Is Implemented": false
    },
    {
        "id": "BW-010",
        "Name of Method": "Rule-based Parsing",
        "Category": "Rule-based",
        "Library/Tools": "Python nltk",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Extracts structured patterns using grammar or parsing techniques.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Rule-based parsing applies structured rules to segment text into meaningful parts. It is effective for formal or structured text but struggles with informal language or slang.",
        "Example Case": "Extracting structured details like \"Education\" and \"Experience\" sections from resumes.",
        "Is Implemented": false
    },
    {
        "id": "BW-011",
        "Name of Method": "Regular Expression Trees",
        "Category": "Rule-based",
        "Library/Tools": "Python pyparsing",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Uses tree structures of regex patterns for more complex rule-based classification.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Extends basic regex patterns into hierarchical structures for handling nested or complex rules. It provides scalability and better organization for intricate patterns.",
        "Example Case": "Extracting details from nested error logs in software debugging systems.",
        "Is Implemented": false
    },
    {
        "id": "BW-012",
        "Name of Method": "Naive Statistical Rules",
        "Category": "Statistical",
        "Library/Tools": "Python, R",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Uses simple statistical thresholds to classify text.",
        "AI/Non-AI": "Non AI",
        "Long Description": "This method uses basic statistics, like word probabilities or frequency thresholds, to classify or filter text. It is suitable for simple datasets but lacks depth for complex or ambiguous input.",
        "Example Case": "Identifying low-priority emails based on the sparse occurrence of critical keywords like \"urgent.\"",
        "Is Implemented": false
    },
    {
        "id": "BW-013",
        "Name of Method": "String Matching (Levenshtein)",
        "Category": "Rule-based",
        "Library/Tools": "Python Levenshtein",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Matches text with predefined templates using edit distance.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Compares strings against templates using similarity metrics like Levenshtein distance to identify matches despite variations or typos.",
        "Example Case": "A search system matches the user query \"defination\" with the correct word \"definition\" for better accuracy.",
        "Is Implemented": false
    },
    {
        "id": "BW-014",
        "Name of Method": "Pattern Templates",
        "Category": "Rule-based",
        "Library/Tools": "Custom scripts",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Uses pre-defined templates of sentence structures for classification.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Predefined templates describe sentence structures or patterns for text classification. It is effective for consistent input but requires updates for evolving data formats.",
        "Example Case": "Extracting customer feedback templates like \"The [product] is [rating]\" to categorize reviews.",
        "Is Implemented": false
    },
    {
        "id": "BW-015",
        "Name of Method": "Word Position Rules",
        "Category": "Rule-based",
        "Library/Tools": "Custom scripts",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Identifies the position of specific words relative to others in a sentence.",
        "AI/Non-AI": "Non AI",
        "Long Description": "Analyzes the positional relationship between words in text to infer meaning or assign categories. Effective for structured text but less robust for free-form input.",
        "Example Case": "Parsing legal documents to classify clauses as \"rights\" or \"obligations\" based on word order.",
        "Is Implemented": false
    },
    {
        "id": "BW-016",
        "Name of Method": "Logistic Regression",
        "Category": "Machine Learning",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Classifies text by fitting a logistic regression model to labeled data.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Logistic regression models classify text into categories by finding a linear decision boundary in feature space. It is efficient for binary or multi-class classification but assumes a linear relationship between features and labels.",
        "Example Case": "Classifying emails into categories like \"business\" or \"personal\" based on word frequencies.",
        "Is Implemented": false
    },
    {
        "id": "BW-017",
        "Name of Method": "Naive Bayes Classifier",
        "Category": "Machine Learning",
        "Library/Tools": "Scikit-learn, NLTK",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Uses probabilities and Bayes' theorem for text classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "This method calculates the probability of a label given text features using Bayes' theorem. It is computationally efficient and handles sparse data well but assumes feature independence, which might not always hold in real-world data.",
        "Example Case": "Filtering spam emails by identifying phrases like \"free money\" or \"win now\" with high probabilities.",
        "Is Implemented": false
    },
    {
        "id": "BW-018",
        "Name of Method": "Support Vector Machines (SVM)",
        "Category": "Machine Learning",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Separates text classes using hyperplanes in a high-dimensional space.",
        "AI/Non-AI": "Use AI",
        "Long Description": "SVM finds the optimal hyperplane to separate classes in high-dimensional space. It works well for text data with clear boundaries but requires careful kernel selection for non-linear problems.",
        "Example Case": "Categorizing social media posts into \"positive\" or \"negative\" sentiment classes.",
        "Is Implemented": false
    },
    {
        "id": "BW-019",
        "Name of Method": "k-Nearest Neighbors (kNN)",
        "Category": "Machine Learning",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Assigns a label based on the majority class of the nearest neighbors.",
        "AI/Non-AI": "Use AI",
        "Long Description": "kNN assigns a category to text based on the majority class of its nearest neighbors in feature space. It is simple and interpretable but computationally intensive for large datasets.",
        "Example Case": "Predicting the genre of books based on the similarity of their descriptions to labeled examples.",
        "Is Implemented": false
    },
    {
        "id": "BW-020",
        "Name of Method": "Decision Trees",
        "Category": "Machine Learning",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Builds decision trees to classify text based on feature splits.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Decision trees split data into branches based on feature thresholds, creating an intuitive and interpretable classification framework. They are effective for small to medium-sized datasets but can overfit without pruning.",
        "Example Case": "Classifying customer feedback into \"positive,\" \"neutral,\" or \"negative\" sentiment based on the presence and frequency of specific keywords.",
        "Is Implemented": false
    },
    {
        "id": "BW-021",
        "Name of Method": "Random Forests",
        "Category": "Machine Learning",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Combines multiple decision trees to improve classification accuracy.",
        "AI/Non-AI": "Use AI",
        "Long Description": "This ensemble method builds multiple decision trees and combines their outputs for classification. It is robust to overfitting on larger datasets and handles non-linear relationships effectively.",
        "Example Case": "Classifying product reviews as \"positive,\" \"neutral,\" or \"negative\" based on customer feedback.",
        "Is Implemented": false
    },
    {
        "id": "BW-022",
        "Name of Method": "TF-IDF with ML Models",
        "Category": "Statistical + ML",
        "Library/Tools": "Scikit-learn",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Combines TF-IDF scores with ML models for classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Combining TF-IDF with machine learning classifiers enhances text classification by transforming textual data into weighted numerical features. Models like SVM or Random Forest can then be applied to these features for robust classification.",
        "Example Case": "A job portal categorizes resumes based on the weighted importance of terms like \"Python\" or \"data analysis\" using TF-IDF scores fed into a logistic regression model.",
        "Is Implemented": false
    },
    {
        "id": "BW-023",
        "Name of Method": "Word Embeddings (Word2Vec)",
        "Category": "Neural Networks",
        "Library/Tools": "Gensim, TensorFlow",
        "Supervised/Unsupervised": "Unsupervised/Supervised",
        "Short Description": "Uses vector representations of words to find patterns for classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Word2Vec embeds words into a vector space to represent semantic relationships. These embeddings are used as features in machine learning classifiers like SVM or logistic regression.",
        "Example Case": "Classifying tweets as \"promotional\" or \"personal\" by leveraging semantic similarities in the text.",
        "Is Implemented": false
    },
    {
        "id": "BW-024",
        "Name of Method": "Recurrent Neural Networks (RNN)",
        "Category": "Deep Learning",
        "Library/Tools": "TensorFlow, PyTorch",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Processes sequential text for context-aware classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "RNNs process sequential data by maintaining a memory of previous inputs, enabling them to understand context and dependencies in text. They are effective for short to medium-length sequences but can struggle with long dependencies.",
        "Example Case": "Generating summaries of news articles by understanding and processing sentences sequentially.",
        "Is Implemented": false
    },
    {
        "id": "BW-025",
        "Name of Method": "Long Short-Term Memory (LSTM)",
        "Category": "Deep Learning",
        "Library/Tools": "TensorFlow, PyTorch",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "A type of RNN designed to handle long-term dependencies in text.",
        "AI/Non-AI": "Use AI",
        "Long Description": "LSTMs are a type of RNN that captures long-term dependencies in sequential data. They use memory gates to retain context over longer sequences, making them effective for complex text classification.",
        "Example Case": "Categorizing customer service chat transcripts into themes like \"technical issue\" or \"billing inquiry.\"",
        "Is Implemented": false
    },
    {
        "id": "BW-026",
        "Name of Method": "Bidirectional LSTMs (BiLSTM)",
        "Category": "Deep Learning",
        "Library/Tools": "TensorFlow, PyTorch",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Uses forward and backward LSTMs for better context understanding.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Extends LSTM by processing text sequences in both forward and backward directions, improving understanding of context and dependencies.",
        "Example Case": "Analyzing movie reviews to classify sentiment based on both preceding and succeeding words in sentences.",
        "Is Implemented": false
    },
    {
        "id": "BW-027",
        "Name of Method": "Convolutional Neural Networks (CNNs)",
        "Category": "Deep Learning",
        "Library/Tools": "TensorFlow, PyTorch",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Extracts features from text using convolutional layers for classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "CNNs, commonly used in image processing, can also classify text by treating sequences of words as spatial data. By applying filters, CNNs extract high-level features from text, making them effective for capturing local dependencies.",
        "Example Case": "Classifying tweets into \"spam\" or \"non-spam\" categories based on specific n-gram patterns detected by convolutional filters.",
        "Is Implemented": false
    },
    {
        "id": "BW-028",
        "Name of Method": "Transformer Models",
        "Category": "Deep Learning",
        "Library/Tools": "Hugging Face Transformers",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Uses transformer architectures like BERT or GPT for advanced text classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Transformers use attention mechanisms to process entire text sequences at once, understanding the relationships between words regardless of distance. They are highly effective for capturing nuanced meanings and are state-of-the-art in many NLP tasks.",
        "Example Case": "Identifying intent in customer service inquiries, such as \"account issues\" or \"product information,\" with high contextual accuracy.",
        "Is Implemented": false
    },
    {
        "id": "BW-029",
        "Name of Method": "Pre-trained Models (BERT)",
        "Category": "Deep Learning",
        "Library/Tools": "Hugging Face Transformers",
        "Supervised/Unsupervised": "Supervised",
        "Short Description": "Fine-tunes pre-trained models on specific datasets for classification.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Transformers like BERT use attention mechanisms to understand the entire context of a text sequence, capturing nuanced relationships and dependencies. It is highly effective but computationally demanding.",
        "Example Case": "Extracting key sections from contracts, such as \"liabilities\" or \"termination clauses,\" with high accuracy.",
        "Is Implemented": false
    },
    {
        "id": "BW-030",
        "Name of Method": "Zero-shot Classification",
        "Category": "Deep Learning",
        "Library/Tools": "Hugging Face Transformers",
        "Supervised/Unsupervised": "Unsupervised",
        "Short Description": "Classifies text into categories without additional training.",
        "AI/Non-AI": "Use AI",
        "Long Description": "Zero-shot classification enables text classification without requiring labeled training data for the target categories. It uses pre-trained language models, like GPT or BART, to predict labels based on natural language descriptions of the classes. This method is powerful for scenarios where data collection is costly or infeasible.",
        "Example Case": "Categorizing customer reviews into new categories like \"environmental concerns\" or \"sustainability\" without specific labeled training examples for these themes.",
        "Is Implemented": false
    }
]